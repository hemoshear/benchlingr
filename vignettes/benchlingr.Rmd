---
title: "benchlingr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{benchlingr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(benchlingr)
library(magrittr)
```


Benchling R&D Cloud is a laboratory information management system with a [developer platform](https://docs.benchling.com/docs) consisting of a Postgres database, an application programming interface (API), an "Events" system that is built on top of the AWS EventBridge service, as well as a Python software development kit (SDK). `benchlingr` is an unofficial R package that aims to make the Benchling developer platform accessible to data scientists using R. 


# Installation

Install the `benchlingr` package from GitHub using the `remotes` package. 

```{r, eval=FALSE}
remotes::install_github("hemoshear/benchlingr")
```

The `benchlingr` package also requires Python. We recommend using the [Anaconda distribution](https://www.anaconda.com/products/distribution)

# Configuration

The `benchlingr` package primarily utilizes the RESTFUL API service and Postgres database components of the Benchling developer platform. In order to use these components of the Benchling platform, one must acquire API keys and database credentials through the Benchling interface. This section will focus on how to make these credentials available to the `benchlingr` package. 

## API

In order to access the Benchling API service on one's Benchling tenant, one must obtain an API key through the Benchling interface. [Follow the instructions in the official documentation](https://help.benchling.com/en/articles/2353570-access-the-benchling-developer-platform-enterprise) to obtain an API key. After obtaining an API key, it is recommended that you define a variable in `.Renviron` called `BENCHLING_API_KEY` to make this key accessible to the `benchlingr` package with the default function arguments. 

```
# ~/.Renviron

# Benchling API key
BENCHLING_API_KEY=xxxxxxxxxxx
```

## Data warehouse

[Follow the instructions in the official documentation](https://docs.benchling.com/docs/getting-started#obtaining-credentials) to obtain a username and password for the data warehouse for your Benchling tenant. After obtaining a username and password, it is recommended that one define a variable in `.Renviron` called `BENCHLING_WAREHOUSE_KEY` and `BENCHLING_WAREHOUSE_PASSWORD` to make the credentials available to `benchlingr` with the default function arguments. 

```
# ~/.Renviron
# Benchling API key
BENCHLING_API_KEY=xxxxxxxxxxx

# Benchling warehouse credentials
BENCHLING_WAREHOUSE_USERNAME=yyyyyyyyyyy
BENCHLING_WAREHOUSE_PASSWORD=xxxxxxxxxxx
```

## Python SDK

The `benchlingr` R package uses the official Benchling Python SDK to access the Benchling API service. The Python dependencies will be installed automatically into a `miniconda` environment. One can also create new `conda` or `virtualenv` environments with the `config_sdk_env` helper function, which simply creates an environment and installs the `benchling-sdk` package in it. 

```{r, eval=FALSE}
config_sdk_env()
```

See the [official Anaconda documentation](linkhere) for instructions on how to install Anaconda on your machine.

# Fundamentals of the Benchling Data Warehouse

The Benchling data warehouse is a read-only, Postgres database that can be used to easily extract data pertaining to items such as the inventory, entities, results, and noteboook entries using SQL queries. Before moving on, we recommend that the user reads the [official Benchling documentation](https://docs.benchling.com/docs/getting-started) for the data warehouse. The rest of this section will assume the user has a basic understanding of Benchling's data warehouse, as we will discuss practical considerations of working with the database, including how to use `benchlingr` tools to streamline your interactions with the database. 

To connect to the data warehouse on your tenant with the `benchlingr` package, use the `warehouse_connect` function. By default, the function looks for two environment variables in `.Renviron`: `BENCHLING_WAREHOUSE_USERNAME` and `BENCHLING_WAREHOUSE_PASSWORD`. If this function is not working for you, please see the instructions for configuration in the  [official documentation](https://docs.benchling.com/docs/getting-started#warehouse-configuration). Once you are able to connect to the database using the instructions in the official tutorial, the `warehouse_connect` function should work properly, assuming that the aforementioned environment variables are set. 

```{r, eval=TRUE}
conn <- warehouse_connect(tenant = "hemoshear-dev",
                          username = Sys.getenv("BENCHLING_DEV_WAREHOUSE_USERNAME"),
                          password = Sys.getenv("BENCHLING_DEV_WAREHOUSE_PASSWORD"))
class(conn)
```

The `warehouse_connect` function will return a Postgres SQL database connection object (`PqConnection`), which can be used to make queries against the database using the `DBI` package in R. To verify that you are connected to the database for your Benchling tenant, one can use the `dbIsValid` to display information about the connection itself. 

```{r, eval=TRUE}
DBI::dbIsValid(conn)
```

The `DBI::dbDisconnect` function closes the warehouse connection. 

```{r}
DBI::dbDisconnect(conn)
```

## "Raw" tables and Postgres views  

One can use the `DBI::dbListTables` function to see all the tables in the database. You'll notice that every "table" in the database is actually comprised of two tables: an unedited version of the data, referred to as the "raw" table, and a cleaned up Postgres view of the data. For example, the `analyte` table is the Postgres view for a custom entity schema we defined, and the `analyte$raw` table is unedited table. 

```{r, eval=TRUE}
conn <- warehouse_connect(tenant = "hemoshear-dev",
                          username = Sys.getenv("BENCHLING_DEV_WAREHOUSE_USERNAME"),
                          password = Sys.getenv("BENCHLING_DEV_WAREHOUSE_PASSWORD"))
DBI::dbListTables(conn)[1:6]
```

In general, the Postgres views are more convenient to work with, since the raw tables contain many Benchling identifiers intended for internal use. In our experience, however, there are some use cases where the raw tables should be accessed instead of the Postgres views. For example, when one needs to extract assay results submitted in notebook entries that have not been approved, one will need to use the "raw" tables to do so, since the Postgres views will not contain assay results from unreviewed notebooks. Refer to the [official Benchling documentation](https://docs.benchling.com/docs/warehouse-columns-rows-descriptions#the-benchling-warehouse-columns-descriptions) for more information about the differences between the raw tables and the Postgres views.  


## Retrieving data from the warehouse  

To retrieve data from a table in the data warehouse, one can use the `DBI::dbGetQuery` function. For example, here is the code one would use to retrieve all the notebook entries from the data warehouse. Take a moment to look at the results of this query, as well as the [descriptions of the columns in the official Benchling documentation](https://docs.benchling.com/docs/warehouse-columns-rows-descriptions).

```{r, eval=TRUE}
DBI::dbGetQuery(conn, "SELECT * FROM entry") %>%
  tail()
```


### Custom entities

The data warehouse for your tenant will come pre-loaded with tables relevant to any entity schemas that are universal to the Benchling system. For example, the `entry` schema is available on every Benchling installation with ELN functionality. When a custom entity or result schema is defined, Benchling creates a new set of tables in the database: one for the Postgres view and one for the raw data. For example, we defined a custom entity schema for representing analytes. 

![Custom "Analyte" entity](img/analyte-screenshot.png){width=85%}
---

When we defined the "Analyte" custom entity schema, Benchling created a table in the data warehouse called `analyte`. All the analyte entities we created can be retrieved with `DBI::dbGetQuery`. 

```{r, eval=TRUE}
DBI::dbGetQuery(conn, "SELECT schema,id,name$,precursor_ion_mz,product_ion_mz FROM analyte") %>%
  head()
```

Benchling also created a "raw" table for our custom analyte schema.

```{r, eval=FALSE}
DBI::dbGetQuery(conn, "SELECT id,schema,precursor_ion_m_z,product_ion_m_z,assay FROM analyte$raw") %>%
  head()
```


`schema`, `id`, and `name$` are important fields present in every entity warehouse table. The `schema` field is the name of the warehouse table itself, which is necessary for some of the critical functionality in `benchlingr`. The `id` field is the Benchling identifier for the entity and the `name$` is the human-readable name for the entity. If a warehouse table $A$ for an entity or results schema contains an entity field, the values in that entity field will be the identifiers for the entity. The information related to all entities referenced in table $A$ will be found in table $B$, where $B$ is the






Our custom "analyte" entity can be associated with one or more "assays", which is a multi-select field. Multi-select fields are represented as JSON objects in the Postgres database, thus the user will want to use one of the many R packages available for working with JSON to coerce the values to an appropriate type for further processing. `RJSONIO` is one package that can be used for coercing a JSON object into a R list or character vector. 

```{r, eval=FALSE}
res <- DBI::dbGetQuery(conn, "SELECT name$,assay FROM analyte")[1,]
res$assay
RJSONIO::fromJSON(res$assay)
```

Custom entities can have fields that are other custom entity types. For example, we defined a custom entity schema for cell cultures, which are connected to a "study" entity that contains the metadata pertaining to an experiment, including a human-readable identifier, who performed the experiment, the objectives of the experiment, etc. 

![Custom "cell culture" entity schema](img/cell_culture_screenshot.png){width=85%}
The `study` field in the `cell_culture` schema is a multi-select, entity field, since we decided that a cell culture can be linked to multiple study entities. 

```{r, eval=FALSE}
cc <- DBI::dbGetQuery(conn, "SELECT schema,id,name$,study FROM cell_culture") %>%
  .[1,]
cc
```

In this example, only one study is linked to this particular cell culture, so we can simply use `RJSONIO::fromJSON` to coerce the `study` column to a more user-friendly character vector. 

```{r, eval=FALSE}
cc$study <- RJSONIO::fromJSON(cc$study)
cc$study
```

Now that we have the identifier for the study cleaned up, we can use this identifier to extract the information in the data warehouse that pertains to the study. 

```{r, error=TRUE, eval=FALSE}
DBI::dbGetQuery(conn, "SELECT * FROM study WHERE id = 'bfi_ZAZ84eD3'")
```
The code above did not work, because the warehouse table for "study" is not actually called `study`. In this case, the field in the "cell culture" table is called "study", but the corresponding entity schema is actually called "study code". 

![study code entity](img/study_code_screenshot.png){width=85%}

The `.map_entity_field_names_to_warehouse_tables` function will take a warehouse data frame as input and return a character vector where the names are the entity column names in the data frame and the values are the warehouse table names. In this example, the `study` column in the `cc` data frame is connected to the `study_code` warehouse table. 

```{r, eval=FALSE}
.map_entity_field_names_to_warehouse_tables(conn, cc)
```


The identifier for any custom entity will start with `bfi_`. To find more information about a custom entity, one can search the data warehouse by ID:

```{r, eval=FALSE}
DBI::dbGetQuery(conn, "SELECT id,name$ FROM study_code WHERE id = 'bfi_ZAZ84eD3'")
```

### Multi-select columns

Fields in entities and results can be configured such that multiple values can be submitted. These "multi-select" columns are represented as JSON objects in the data warehouse. To unpack the values in these columns to create new rows or columns in the data frame, use the `expand_multiselect_column` function. 

To unpack the value such that the original row is copied for each value in the multi-select column, use the `shape="long"` argument. 

```{r, eval=FALSE}
new_df <- expand_multiselect_column(conn, df, column="analyte", shape="long")
```

To unpack the value into new columns in the original row, use the `shape="wide"` argument. By default, the new columns are named by appending a number ($(1, 2, ..., n)$, where n is the maximum number of elements that appear in a value within the multi-select field) to the original column name.

```{r, eval=FALSE}
new_df <- expand_multiselect_column(conn, df, column="analyte", shape="wide")
```





# Accessing the Benchling API via the Python SDK in R. 

To create an interface to the Benchling API, use the `benchling_api_auth` function after you have set the `BENCHLING_API_KEY` environment variable as described in the Python SDK configuration section. 

```{r}
client <- benchling_api_auth(tenant="https://hemoshear-dev.benchling.com",
                             api_key=Sys.getenv("BENCHLING_DEV_API_KEY"))
```

The `benchling_api_auth` function returns the [facade object](https://benchling.com/sdk-docs/benchling_sdk.benchling.html) from the Benchling Python SDK that facilitates interaction with the API. Most of the attributes of the facade object are the [services in the Benchling API documentation](https://benchling.com/api/reference). 


```{r}
head(names(client))
```

Each service will have multiple methods available. 

```{r}
names(client$entries)
```

# Notebook Entries

Notebook entries can be found in the `entry` table in the data warehouse. Note, this table contains some of the notebook entry metadata, but not the content of the entry itself.

```{r}
conn <- warehouse_connect("hemoshear-dev",
                          username = Sys.getenv("BENCHLING_DEV_WAREHOUSE_USERNAME"),
                          password = Sys.getenv("BENCHLING_DEV_WAREHOUSE_PASSWORD"))
DBI::dbGetQuery(conn, "SELECT id,display_id,schema_id,entry_template_id,review_status FROM entry") %>%
  head()
```

If the notebook entry uses a custom metadata schema, that metadata can also be found in the data warehouse. For example, we have a "Study Plan Entry" notebook schema for a certain type of notebook entry. The warehouse table for this schema is `study_plan_entry`.

```{r, eval=FALSE}
DBI::dbGetQuery(conn, "SELECT id,study_code FROM study_plan_entry") %>%
  head()
```


The content of the notebook entries (aside from results and entities), must be retrieved from the API. To retrieve a notebook entry, use the `entries$get_entry_by_id` method. The method takes the ID of an entry as an argument, which starts with `etr_`. 

```{r}
entry <- client$entries$get_entry_by_id("etr_T3WZTyAe")
```

```{r}
class(entry)
```


## Reading unstructured tables.  

Benchling users can record information in unstructured tables within notebook entries. 

![Example of an unstructured table in a Benchling notebook entry](img/unstructured_entry_tables.png)
The `read_entry_tables` function extracts all unstructured tables in a notebook entry, and returns them as a list of data frames for further processing in R. 

```{r}
tables <- read_entry_tables(entry)
tables[[1]]
```


## Reading plate diagrams.  

In addition to unstructured tables, Benchling users can create "plate diagrams" to describe the contents of physical plates. The plate diagram tables can only be certain sizes (2x3, 3x4, 4x6, 6x8, 8x12, 16x24, and 32x48), the column names must be integers, and the row names must be letters. 

![Plate diagrams in a Benchling notebook entry.](img/plate_diagram_entry.png)

It is useful to convert these tables into long form for further processing in R. To do so, one can start with the `read_plate_diagrams` function instead of the `read_entry_tables` function. The `read_plate_diagrams` will extract the plate diagrams from the notebook entry and convert the output data frames into long form tables with three columns: row, column, and content. The `row` is a letter for the row on the plate, the `column` is an integer for the column on the plate, and `content` is the text that appears in the cell in the table. 

The names of the list will be the names of the tables in the notebook entry.  

```{r}
entry <- client$entries$get_entry_by_id("etr_f1bpDIes")
diagrams <- read_plate_diagrams(entry)
str(diagrams)
head(diagrams$Well1)
```

## Schemas within notebook entries  

Both entities and results can be created in tables within notebook entries. A data analyst will need to query a specific API endpoint or warehouse table to find the registered entities or results, so one needs to know the schema associated with the entity or results table. The Benchling user can change the name displayed for the tables in the notebook entry, so it is not always obvious which schemas are present in the notebook entry. Furthermore, if the data will be extracted from the warehouse, we'll need to know the names of the warehouse tables to query, which cannot always be inferred from the schema names alone. 

```{r, eval=FALSE}
list_schemas_in_entry("etr")
```


# Downloading attached files

Benchling users can attach files to results and entities, which are referred to as blobs in the backend.

![A results table in a Benchling notebook entry with files attached.](img/blobs_in_table.png)  

----

The schema name for this example table is `simple_plate_analyte_mapping`, so we can query the table to get the identifiers and file names for the blobs to download. 

```{r, eval=TRUE}
d <- DBI::dbGetQuery(conn, "SELECT plate,analytes,file FROM simple_plate_analyte_mapping$raw WHERE entry_id$ = 'etr_JYUlMiIs'")
d
```

Each "blob" consists of an identifier, a URL, and a name. `download_blobs` is a low-level wrapper for the Python SDK that downloads a set of blobs from Benchling as specified by their ID and file name. 

```{r, eval=FALSE}
download_blobs(
  client, 
  file_map = list(
    "ff0cca5f-b400-4e42-9df6-9f1badc4b7e2" = "Plate1-Data.csv",
    "a2ed3ec8-59b9-451a-81eb-c4fb5322858b" = "Plate2-Data.csv"),
  outdir='data')
```

The `download_blobs_in_warehouse_table` function will download all files attached to a particular set of columns in a warehouse table. By default, it will try to download files from all blob columns unless a specific set is passed to the `column` argument. The files in each column are saved to a subdirectory within `outdir`. 

```{r, eval=FALSE}
download_blobs_in_warehouse_table(conn, d, column='file', outdir="data")
```



```{r, eval=FALSE, echo=FALSE}
library(ggplot2)
ggplot(diagrams$Plate1, aes(x=column, y=row, label=content)) +
  geom_tile() +
  geom_text() +
  theme(axis.text.x = element_text(position='top'))
ggplot2::text
```



